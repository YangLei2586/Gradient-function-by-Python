# *Gradient-function-by-Python*
## a simple sample of gradient and the numbers are asigned randomly. We will be using the sigmoid as the activation function.The learning rate is set to 0.5.
## steps
*initial weights
*calculate one gradient descent step for each weight 
*calculate error of neural network
*calculate change in weights
* print out the _'Neural Network output_'
* print out the _'Amount of Error'_
* print out the _'Change in Weights'_
